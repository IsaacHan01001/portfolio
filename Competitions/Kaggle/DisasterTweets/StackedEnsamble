import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import TextVectorization, Embedding, GlobalAveragePooling1D, Dense, Input
from tensorflow.keras import Model
from tensorflow.keras.callbacks import EarlyStopping

import random
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

path = os.path.join(".", "_data", "kaggle", "Disaster_Tweets")
pathTrain = os.path.join(path, "train.csv")
pathTest = os.path.join(path, "test.csv")
pathSubmission = os.path.join(path, "sample_submission.csv")

train_df = pd.read_csv(pathTrain)
test_df = pd.read_csv(pathTest)
submission_df = pd.read_csv(pathSubmission)
labels = train_df.pop("target")
# print(train_df.isna().sum())
# print(test_df.isna().sum())

r"""
id             0
keyword       61
location    2533
text           0
dtype: int64

id             0
keyword       26
location    1105
text           0
dtype: int64

Submission file 'submission.csv' created successfully!
   id  target
0   0       0
1   2       0
2   3       1
3   9       1
4  11       1
PS C:\Users\Isaac_Han\Desktop\CS\IBM_RedHat> & C:/Users/Isaac_Han/anaconda3/envs/tf273gpu/python.exe c:/Users/Isaac_Han/Desktop/CS/IBM_RedHat/Keras/keras67_NLP_disasterTweet_ensamble.py
   id keyword location                                               text
0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...
1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada
2   5     NaN      NaN  All residents asked to 'shelter in place' are ...
3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...
4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...
"""
# exit()
# Create the TextVectorization layer

settings = dict()
settings["keyword"] = {"max_tokens": 30, "output_mode" : "int", "output_sequence_length" : 100}
settings["location"] = {"max_tokens": 50, "output_mode" : "int", "output_sequence_length" : 100}
settings["text"] = {"max_tokens": 10000, "output_mode" : "int", "output_sequence_length" : 600}
colNames = ["keyword", "location", "text"]
vectorizers = []
for colName in colNames:
    train_df[colName].fillna("Unknown", inplace = True)
    test_df[colName].fillna("Unknown", inplace = True)
    temp = TextVectorization(
        **settings[colName]
    )
    vectorizers.append(temp)

for idx, vectorizer in enumerate(vectorizers):
    vectorizer.adapt(train_df[colNames[idx]])

models = []
for idx, colName in enumerate(colNames):
    temp = Sequential([
        tf.keras.Input(shape=(1,), dtype=tf.string),
        vectorizers[idx],
        Embedding(input_dim=settings[colName]["max_tokens"], output_dim=128),
        GlobalAveragePooling1D(),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    models.append(temp)

for model in models:
    model.compile(
        loss='binary_crossentropy',      # The best loss function for binary (0/1) problems.
        optimizer='adam',                # A popular and effective optimizer.
        metrics=['accuracy']             # We want to monitor accuracy during training.
    )

es = EarlyStopping(
    monitor = "val_loss", 
    mode = "min", 
    patience = 20,
    restore_best_weights=True,
)

for idx, colname in enumerate(colNames):
    models[idx].fit(
        train_df[colname],
        labels,
        epochs=1000,                          # Number of times to go through the entire dataset.
        validation_split=0.2,              # Use 20% of the data for validation during training.
        batch_size=32,                      # Number of samples per gradient update.
        callbacks = [es]
    )
train_preds = []
for idx, model in enumerate(models):
    train_preds.append(model.predict(train_df[colNames[idx]]))
stacked_train = np.hstack(train_preds)

preds = []
for idx, model in enumerate(models):
    preds.append(model.predict(test_df[colNames[idx]]))

stacked_preds = np.hstack(preds)

#meta model
input_meta = Input(shape=(3,))
x = Dense(8, activation="relu")(input_meta)
output_meta = Dense(1, activation="sigmoid")(x)
meta_model = Model(inputs=input_meta, outputs=output_meta)
meta_model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
meta_model.fit(stacked_train, labels, epochs = 1000, batch_size = 22, validation_split = 0.2, callbacks = [es])

final_preds = meta_model.predict(stacked_preds)
final_labels = (final_preds > 0.5).astype(int)

# Create the submission file in the required format.
submission_df['target'] = final_labels
submission_df.to_csv(os.path.join(path, 'stacked_ensamble_submission.csv'), index=False)

print("\nSubmission file 'submission.csv' created successfully!")
print(submission_df.head())
